# WebSearch MCP RAG - Dockerized AI Assistant

A complete dockerized implementation of Model Context Protocol (MCP) for autonomous web search, content extraction, and Retrieval-Augmented Generation (RAG) using ChromaDB and local LLMs.

## ğŸ—ï¸ Architecture

This system implements MCP 2025 specification with dynamic tool selection, allowing AI models to autonomously decide which tools to use based on the query context.

```
websearch_mcp/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ client/              # MCP Client for LLM integration  
â”‚   â”‚   â”œâ”€â”€ mcp_client.py    # Main MCP client with dynamic tool calling
â”‚   â”‚   â””â”€â”€ mcp_client_config.json # Client configuration
â”‚   â””â”€â”€ mcp/                 # MCP Server exposing RAG tools
â”‚       â””â”€â”€ mcp_server.py    # Server with web_search, extract_content, rag_search tools
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ extractor/           # Content extraction service (FastAPI)
â”‚   â”‚   â”œâ”€â”€ app.py           # Trafilatura-based content extraction
â”‚   â”‚   â”œâ”€â”€ Dockerfile       # Container definition
â”‚   â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”‚   â”œâ”€â”€ vectorstore/         # ChromaDB vector database service
â”‚   â”‚   â”œâ”€â”€ content_vectorizer.py # Smart chunking & embedding
â”‚   â”‚   â”œâ”€â”€ Dockerfile       # Container definition  
â”‚   â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ websearch/           # Web search service wrapper
â”‚       â””â”€â”€ websearch_service.py # SearXNG integration
â”œâ”€â”€ data/
â”‚   â””â”€â”€ chroma_db/           # Persistent vector database
â”œâ”€â”€ config/
â”‚   â””â”€â”€ searxng/
â”‚       â””â”€â”€ settings.yml     # SearXNG search engine configuration
â”œâ”€â”€ docker-compose.yml       # Complete service orchestration
â”œâ”€â”€ Dockerfile              # MCP client container
â”œâ”€â”€ START_MCP_RAG.BAT       # Windows launch script
â””â”€â”€ requirements.txt        # Main dependencies
```

## ğŸ”§ Services Architecture

### Core Services
1. **Redis** - Caching and session management
2. **SearXNG** - Privacy-focused search engine
3. **Extractor** - Advanced content extraction using Trafilatura
4. **Vectorstore** - ChromaDB with intelligent chunking and embeddings  
5. **MCP Client** - Dynamic tool orchestration with local LLMs

### MCP Tools (Auto-Selected by AI)

#### ğŸ§  **Intelligent Tools (AI Auto-Selects Based on Query)**
- **`research_query`** - Comprehensive research with parallel searches, smart content processing, and RAG synthesis
- **`smart_answer`** - Intelligent answering: checks knowledge base first, supplements with web search when needed

#### ğŸ”§ **Core Tools (Building Blocks)**  
- **`web_search`** - Search internet for current information (with Redis caching)
- **`extract_content`** - Extract full text from webpages 
- **`rag_search`** - Search stored knowledge base with semantic similarity
- **`store_content`** - Store content in vector database with smart chunking
- **`knowledge_stats`** - View knowledge base statistics and health

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- Ollama running on host machine (port 11434)
- At least 8GB RAM recommended

### 1. Launch the System
```bash
# Windows
START_MCP_RAG.BAT

# Linux/Mac  
docker-compose up --build
```

### 2. Supported Models
The system works with any Ollama model:
- `gpt-oss:20b` (default) 
- `qwen3:0.6b`
- `qwen3:8b`
- `llama3:latest`
- Any other Ollama model

### 3. Intelligent Query Processing

The AI automatically chooses the optimal strategy based on your query:

#### ğŸ” **Simple Questions â†’ `smart_answer`**
```
You: "What is Tesla's current stock price?"

ğŸ¤” Analyzing query...
ğŸ“š Checking knowledge base... (no recent data)
ğŸ” Getting current web info...
ğŸ“„ Direct answer with source (content < 2KB)

ğŸ¤– Tesla (TSLA) is currently trading at $248.50...
Source: https://finance.yahoo.com/quote/TSLA
```

#### ğŸ§  **Complex Research â†’ `research_query`**  
```
You: "Research the latest developments in quantum computing and store findings"

ğŸ¤” Complex research query detected...
ğŸ”§ research_query: parallel searches across multiple sources
ğŸ”— Found 3 sources
ğŸ“„ Extracting content in parallel...
ğŸ’¾ Stored 2 large articles (>500 chars each)  
ğŸ“„ Direct summary from 1 small article
ğŸ“š RAG synthesis from stored content

ğŸ¤– Comprehensive quantum computing report with:
- Current breakthrough summaries
- Stored knowledge synthesis  
- Full source attribution
```

#### ğŸ“š **Knowledge Questions â†’ RAG-First**
```
You: "What do we know about Tesla from our previous research?"

ğŸ¤” Knowledge-based query...
ğŸ“š High relevance match found (similarity: 0.89)
âœ… Answered from knowledge base only

ğŸ¤– Based on stored research: Tesla's Q3 2024 results showed...
Sources: 3 previously stored articles
```

## ğŸ§  Advanced Features

### Dynamic Tool Selection & Content Processing
Based on MCP 2025 specification with intelligent decision making:

#### ğŸ¯ **Query Analysis & Tool Routing:**
- **Simple/Direct questions** â†’ `smart_answer` (RAG-first, web supplement)
- **Complex research** â†’ `research_query` (parallel search + extraction + RAG)
- **Knowledge-only queries** â†’ `rag_search` (pure knowledge base)
- **Specific URL extraction** â†’ `extract_content` + conditional storage

#### ğŸ“Š **Content Size Intelligence:**
- **Small content (<2KB)** â†’ Direct consumption and immediate response
- **Medium content (2KB-10KB)** â†’ Store in knowledge base + provide summary  
- **Large content (>10KB)** â†’ Smart chunking + embedding + RAG synthesis
- **Too large (>10MB)** â†’ Truncation with warning

#### âš¡ **Parallel Processing with Redis Coordination:**
- **Multiple URLs** â†’ Concurrent extraction via asyncio.gather()
- **Search caching** â†’ 30-minute TTL to avoid duplicate searches
- **Duplicate prevention** â†’ Content hash checking before storage
- **Load balancing** â†’ Redis-coordinated task distribution

### Intelligent Content Processing
- **Smart Chunking** - Semantic boundary detection
- **Embedding Generation** - Using Ollama's nomic-embed-text
- **Similarity Search** - Cosine similarity with configurable thresholds
- **Source Tracking** - Full provenance and citation management

### Data Persistence
- **ChromaDB** - Persistent vector storage in `./data/chroma_db/`
- **Redis** - Fast caching and session data
- **Docker Volumes** - Automatic data persistence across restarts

## ğŸ” Example Workflows

### 1. Research Assistant
```
"Research the company Tesla's recent quarterly results and store the findings"
â†’ web_search â†’ extract_content â†’ store_content â†’ summarize
```

### 2. Knowledge Synthesis  
```  
"What do we know about climate change impacts from our knowledge base?"
â†’ rag_search â†’ synthesize from stored content
```

### 3. Fact Verification
```
"Is this claim about AI development accurate: [claim]"
â†’ rag_search (stored knowledge) â†’ web_search (current info) â†’ compare
```

## ğŸ“Š Monitoring & Health

- **Health Checks** - All services have built-in health monitoring
- **Service Dependencies** - Proper startup order and dependency management
- **Logging** - Comprehensive logging across all services
- **Performance** - Optimized for low-latency tool selection

## ğŸ› ï¸ Configuration

### Environment Variables
```yaml
# MCP Client
OLLAMA_URL: "http://host.docker.internal:11434"
WEBSEARCH_URL: "http://extractor:8055"

# Extractor Service  
MAX_CONTENT_LENGTH: "10485760"  # 10MB
REQUEST_TIMEOUT: "30"

# SearXNG
SEARXNG_SECRET: "change_this_searxng_secret_key"
```

### Customization
- **Search Engines** - Modify `config/searxng/settings.yml`
- **Embedding Models** - Change model in `content_vectorizer.py`
- **Chunking Strategy** - Adjust parameters in smart_chunk()
- **Tool Behavior** - Customize MCP tools in `mcp_server.py`

## ğŸ“ˆ System Requirements

### Minimum
- 4GB RAM
- 2 CPU cores  
- 10GB disk space

### Recommended
- 8GB+ RAM
- 4+ CPU cores
- 50GB+ disk space (for vector storage)
- SSD storage for better performance

## ğŸ” Security Notes

- SearXNG runs with minimal privileges
- Content extraction has size limits
- No external API keys required
- All data stored locally

## ğŸ¤ Model Context Protocol (MCP) 2025

This implementation follows MCP 2025 specification:
- **Dynamic Tool Discovery** - Runtime tool registration and updates
- **Resource Management** - Efficient context and resource handling  
- **Standardized Interface** - Universal AI-tool communication
- **Extensible Architecture** - Easy addition of new tools and services

The AI model autonomously decides which tools to use based on the query, making it truly autonomous and context-aware.